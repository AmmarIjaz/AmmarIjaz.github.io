This way of framing things makes it much easier to think about the Replication Crisis sweeping through the social sciences.

One of the causes of it is p-values, which is the probability that the results you got from your study are because of chance vs because of an actual effect.

A p-value closer to 1 means your result is because of random chance and closer to 0 means it's not chance, but caused by an actual effect.

The arbitrary cutoff of 0.05 was adopted at some point. You can see the problem with it once you convert it to percentage by multiplying by 100—5%—then convert to Natural Frequency—1 in 20.

This means that 1 out of 20 times you run this study, you'll get a "false positive"—the appearance of an effect that actually came about by chance but _looks_ like it came about because of a result. That's not a very low probability.

Let's say twenty different researchers did run the study, but because of a bias towards submitting and publishing positive findings, only one of them find a positive effect and published it, while the other 19 didn't.
